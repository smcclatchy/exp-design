---
title: "Linear regression and analysis of variance"
teaching: 0
exercises: 0
questions:
- "?"
- "?"
objectives:
- "Understand the assumptions behind linear models and ANOVA."
- "Perform ANOVA and interpret results."
keypoints:
- "When performing a linear regression or ANOVA, ensure that data meet the assumptions for this chosen method."
source: Rmd
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("05-")
```

## Linear Regression
Linear regression describes how one variable changes dependent on another.
We can use linear regression to predict the mean value of a dependent, or response variable according to the value of an independent variable, or predictor. In the following example we'll explore how body weight changes depending on diet composition. 

## Data

This data set contains body weights for 846 male (M) and female (F) mice fed a high fat (hf) and control (chow) diet measured repeatedly over 23  time points (BW.3 – BW.30).  Many mice do not have measurements for all body weight as survival time varies across sample.  Data set includes additional information, such as litter number and coat color. Load the data and have a quick look.

```{r load_data}
pheno <- read.csv(file = "bodyWeights.csv", stringsAsFactors = FALSE)
names(pheno)
head(pheno)
```

The data are from a repeated measures design, in which the same subject is measured over time. Although this data represents a repeated measures experiment, it can also be analyzed as ANOVA (Analysis of Variance) by evaluating only a single time point’s body weight measurement.  Due to the additional complexity of a repeated measures experiment over a standard ANOVA, the following statistical analysis example will focus only on the analysis of body weights at 10 weeks (BW.10).

## Statistical Analysis Assumptions (of Linear Regression / ANOVA)
To ensure that a statistical analysis can accurately evaluate a data set, there are certain criteria (or assumptions) that need to be met.

For our analysis of body weight at 10 weeks and diet, the following assumptions should be met:

1. The model is good (i.e. the relationship is linear and not, for example, quadratic or exponential).  
1. The residuals have a normal distribution.  
1. The residuals have equal variance (homoscadastic).  

In the following example, we model body weight at 10 weeks as a function of diet.

![](../fig/linear-model.png)

The residuals (or errors) are the distance of each data point from the line describing the linear model, as shown above. If we look at a histogram of the residuals, they should be normally distributed.

![](../fig/residual-histogram.png)

Let's create the model and explore the residuals and fitted values.

```{r linear_model}
# Model body weight at 10 weeks as a function of diet
model <- lm(BW.10 ~ Diet, data = pheno)
summary(model)
# Look at the first few values of the residuals
head(model$residuals)
# Look at the first few values fitted by the model
head(model$fitted.values)
```

Note that the first several fitted values are the same. These are the predicted mean body weights for a high-fat  diet. What are the predicted values for a standard chow diet?

```{r hf_fitted_values}
table(model$fitted.values)
```

The difference between predicted values for standard chow vs. high-fat diet is approximately `r round(table(model$fitted.values)[4] - table(model$fitted.values)[1], digits = 1)`, which is the slope of the line describing the linear model. 

Now create a histogram of the residuals to check for a normal distribution.

```{r histogram, eval=FALSE}
hist(model$residuals, breaks = 20)
```

You can also plot the residuals against the fitted values in the model.

```{r resid_vs_fitted, eval=FALSE}
plot(model, which = 1)
```

Note that the residuals are plotted along one of two fitted values - the one for standard chow (25.9), or the predicted value for high-fat diet (28.5). There should be constant variance vertically and points should scatter symmetrically around zero.  

We can also use a quantile vs. quantile (Q-Q) plot to compare the residuals to a normal distribution. 

```{r qq_plot, eval=FALSE}
qqnorm(model$residuals)
qqline(model$residuals)
```

The Q-Q plot 

## The Model

Evaluating the assumptions of the statistical test requires that a model be created.  A statistical model is a mathematical representation of the factors that can be used to predict a certain value.  For example, in our effort to predict BW.10 data, we are using the sex and diet of mice which are represented in the form of a statistical model.

The first step in analyzing data is to create an appropriate model.  Given our data set, we would like to determine if the dependent variable of body weight (BW.10) is influenced by the independent variables sex and diet.  The model for this analysis is:

![](../fig/bw-sex-diet-eqn.png)

where,
<i>y</i> = <i>&alpha;</i> + <i>&beta;X</i> + <i>&epsilon;</i>

<i>y<sub>i</sub></i> = the dependent (or response variable), Body Weight (BW.10), associated with sample <i>i</i>.

The subscript <i>i</i> refers to the individual sample

In our data set, the <i>i</i> subscript refers to the ID in the Sample column of the data set.

The response variable, <i>BW.10</i>, is a quantity that varies in a way that we hope to be able to summarize and exploit via the modeling process. Generally, it is known that the variation of the response variable is systematically related to the values of one or more other variables (such as, Sex and Diet) before the modeling process is begun, although testing the existence and nature of this dependence is part of the modeling process itself.

The mathematical function consists of two main parts. These parts are known as the predictor variables (or regressors), e.g., <i>sex<sub>i</sub></i>,… , and the parameters (or regression coefficients), e.g., <i>&beta;<sub>1</sub></i>,….

The below parameters (or regression coefficients) are constants that do not change according to sample, sex, or diet.

<i>&beta;<sub>0</sub></i> = mean intercept (or constant); for scientific studies the intercept is often not of interest and is only used to aid in calculation of predicted values

<i>&beta;<sub>1</sub></i> = parameter associated with the regressor Sex

<i>&beta;<sub>2</sub></i> = parameter associated with the regressor Diet

The parameters are the quantities that will be estimated during the modeling process. Their true values are unknown and unknowable, except in simulation experiments.

The relationship (or parameter) between BW.10 and Sex and Diet is the same regardless of which sample is evaluated.

<i>sex<sub>i</sub></i> = a regressor that varies according to the ith sample’s sex

<i>diet<sub>i</sub></i> = a regressor that varies according to the ith sample’s diet

The predictor (or regressor) variables are observed along with the dependent (or response) variable, BW.10.

<i>&epsilon;<sub>i</sub></i> = error (or residual) associated with observation i

Like the parameters (or regression coefficients) in the mathematical function, the random errors are unknown. The error (or residual) is simply the difference between what is seen in the data set versus what is predicted by the mathematical function.


Include R example of using aov() to setup an analysis of variance to predict BW.10 using both Sex and Diet factors.  May wish to mention the use of “lme” when you need to account for both fixed and random factors, such as when a random term is required for accounting for technical replicates (or other factors).


## Residual vs. Fitted Plots

A model can be assessed using the residual vs. fitted (or predicted) values plot.  Below is an example of a good (top) and bad (bottom) residual vs. fitted values plot.  Trends (such as a “V” shape) are to be avoided because they possibly indicate nonlinear data.

Include the plot for the BW.10 data.

Find high-quality image of an example of a bad plot.


A residual by predicted plot is commonly used to diagnose nonlinearity or nonconstant error variance. Additionally, it is also used to find outliers (data points that greatly deviate from all other points).


## Outliers

If outliers are believed to be present in the data, data transformation may be considered.  Alternatively, if outlier values are believed to be the result of real error (e.g. calculation error, data entry, etc.) then they may be removed from the dataset.  Excluding values must only be done for legitimate reasons, or else you may affect the Type 1 (false positive) error rate.

Include plot of distribution of data (include R box plot code).  Highlight any outliers.  Go over how to read a box plot.  Show example of outliers if none are present in data.


## The residuals

Residuals are estimates of experimental error obtained by subtracting the observed responses from the predicted responses (or actual data from data set minus what is predicted by the model).  The predicted response is calculated from the chosen model, after all the unknown model parameters have been estimated from the experimental data.  Examining residuals is a key part of all statistical modeling.  Carefully looking at residuals can tell us whether our assumptions are reasonable and our choice of model is appropriate.

Residuals are elements of variation unexplained by the fitted model.  Residuals should be (roughly) normal and (approximately) independently distributed with a mean of zero and some constant variance.  If error is not normal or independently distributed this would indicate that a different (nonlinear) model may be more suitable to analyze the data or that other significant factors need to be accounted for.  For example, if predicting BW.10 we only used a model with Sex, we may obtain poor residual plots because we are failing to account for a crucial factor, such as, Diet.  Show example of residual plots only using Sex (and provide the R code).

## 2.2.1 Normal Probability Plot

Residual normality can be evaluated via a QQ (quantile-quantile plot).  Provide an example of a QQ plot and R code.

## Failures
fail to log transform data - produce bar plots that start at zero and  with error bars successively larger error bars, statistical significance shown - see Science p.1091 8 june 2018, also p. 1128
ttest or anova assumes equal variances, assuming same here
makes data normal, stabilizes variance

another example - throwing out outliers