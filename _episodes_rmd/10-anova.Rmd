---
title: "Linear regression and analysis of variance"
teaching: 0
exercises: 0
questions:
- "How do I evaluate the quality of a linear model?"
- "?"
objectives:
- "Understand the assumptions behind linear models and ANOVA."
- "Perform linear regression and interpret results."
- "Evaluate model quality by examining the residuals."
- "Know when and how to transform your data."
keypoints:
- "When performing a linear regression or ANOVA, ensure that data meet the assumptions for this chosen method."
- "Examining the residuals (or errors) is a key part of linear regression."
- "Transform your data if it appears to have many outliers or would benefit from a transformation such as logarithmic."

source: Rmd
---

```{r, include=FALSE}
source("../bin/chunk-options.R")
knitr_fig_path("10-")
```

## Linear Regression
Linear regression describes how one variable changes dependent on another one, or two, or more variables. For example, we can use linear regression to model human height as a function of gender.





The linear model describes the line of best fit to the data, and can be expressed as an equation <i>y</i> = <i>&beta;<sub>0</sub></i> + <i>&beta;<sub>1</sub></i>x + <i>&epsilon;</i>

where 

&nbsp;&nbsp;&nbsp;<i>y</i> is the dependent variable, height
&nbsp;&nbsp;&nbsp;<i>&beta;<sub>0</sub></i> is the y-axis intercept
&nbsp;&nbsp;&nbsp;<i>&beta;<sub>1</sub></i> is the slope of the line
&nbsp;&nbsp;&nbsp;<i>x</i> is the independent variable, gender, and
&nbsp;&nbsp;&nbsp;<i>&epsilon;</i> is the error (also called residual)

This is similar to the equation <i>y</i> = <i>m</i><i>x</i> + <i>b</i> that you might be familiar with from many years ago.

We can use linear regression to predict the mean value of a dependent, or response variable according to the value of one or more independent variables, or predictors. In the following example we'll explore how body weight (the dependent variable) changes depending on diet composition (the independent variable). 

## Data

This data set contains body weights for 846 male (M) and female (F) mice fed a high fat (hf) and control (chow) diet measured repeatedly over 23  time points (BW.3 – BW.30).  Many mice do not have measurements for all body weight as survival time varies across sample.  Data set includes additional information, such as litter number and coat color. Load the data and have a quick look.

```{r load_data}
library(car)
pheno <- read.csv(file = "../data/bodyWeights.csv", stringsAsFactors = FALSE)
names(pheno)
head(pheno)
```

The data are from a repeated measures design, in which the same subject is measured over time. Although this data represents a repeated measures experiment, it can also be analyzed as ANOVA (Analysis of Variance) by evaluating only a single time point’s body weight measurement.  Due to the additional complexity of a repeated measures experiment over a standard ANOVA, the following statistical analysis example will focus only on the analysis of body weights at 10 weeks (BW.10).

## Statistical Analysis Assumptions (of Linear Regression / ANOVA)

In the following example, we model body weight at 10 weeks as a function of diet.

![](../fig/linear-model.png)

The residuals (or errors one if which is shown by the vertical, red-dashed line in the plot above) are the distance of each data point (a light blue circle) from the (blue) line describing the linear model. 

To ensure that a statistical analysis can accurately evaluate a data set, there are certain criteria (or assumptions) that need to be met.

For our analysis of body weight at 10 weeks and diet, the following assumptions should be met:

1. The model is good (i.e. the relationship is linear and not, for example, quadratic or exponential).  
1. The residuals have a normal distribution (*the data don't necessarily need to be normally distributed, but the residuals do*).  
1. The residuals have equal variance (homoscedastic).  

If we look at a histogram of the residuals, they should be normally distributed.  A normal, or Gaussian, distribution is identified by having a mean, median, and mode that are all equal, and the data generate a curve that is symmetric at the center (or mean).

![](../fig/residual-histogram.png)

You can also plot the residuals against the fitted values in the model to check assumption number 3, that the residuals have equal variance.

```{r resid_vs_fitted}
plot(model, which = 1)
```

Note that the residuals are plotted along one of two fitted values - the one for standard chow (25.9), or the predicted value for high-fat diet (28.5). There should be constant variance vertically and points should scatter symmetrically around zero. The plot indicates the 3 data points that stand out as outliers, with index numbers supplied for each.   

Let's create the model and explore the residuals and fitted values.

```{r linear_model}
# Model body weight at 10 weeks as a function of diet
model <- lm(formula = BW.10 ~ Diet, data = pheno)
summary(model)
# Look at the first few values of the residuals
head(model$residuals)
# Look at the first few values fitted by the model
head(model$fitted.values)
```

Note that the first several fitted values are the same. These are the predicted mean body weights for a high-fat  diet. What are the predicted values for a standard chow diet?

```{r hf_fitted_values}
table(round(model$fitted.values, digits = 4))
```

The difference between predicted values for standard chow vs. high-fat diet is approximately `r round(as.numeric(names(table(model$fitted.values))[4]) - as.numeric(names(table(model$fitted.values))[1]), digits = 1)`, which is the slope of the line describing the linear model. 

## The residuals: a key part of statistical modeling

Residuals are estimates of experimental error obtained by subtracting the observed responses from the predicted responses (or actual data from data set minus what is predicted by the model).  The predicted response (or fitted value) is the dependent variable (y-value) that is predicted by the model based on all the accounted for independent variables (x-values). 

A fitted value is simply another name for a predicted value as it describes where a particular x-value fits the line of best fit. It is found by substituting a given value of x into the regression equation. A residual denoted (e) is the difference or error between an observed observation and a predicted or fit value

Examining residuals is a key part of all statistical modeling.  Carefully looking at residuals can tell us whether our assumptions are reasonable and our choice of model is appropriate.

Residuals are elements of variation unexplained by the model.  Residuals should be (roughly) normal and (approximately) independently distributed with a mean of zero and some constant variance.  If error is not normal or independently distributed this would indicate that a different (nonlinear) model may be more suitable to analyze the data or that other significant factors need to be accounted for.  For example, if predicting BW.10 we only used a model with Sex, we may obtain poor residual plots because we are failing to account for a crucial factor, such as, Diet.  Show example of residual plots only using Sex (and provide the R code).

Now create a histogram of the residuals to check for a normal distribution.

```{r histogram, eval=FALSE}
hist(model$residuals, breaks = 20)
```


We can also use a quantile vs. quantile (Q-Q) plot to compare the residuals to a normal distribution. 

```{r qq_plot}
plot(model, which = 2)
```

The Q-Q plot indicates 3 data points that are outliers along with their index numbers. Otherwise, most of the points lie along the diagonal line, indicating that the residuals are normally distributed. 




## A Bad Model
Now let's look at one that is unmistakably bad. This is a linear model created from a fake dataset. Notice that the slope of the line is nearly horizontal.

```{r bad_data, echo=FALSE}
x <- c(rep(1,300), rep(2, 300))
y <- rgamma(600, shape = 1)
bad_data <- data.frame(x, y)
```

![](../fig/bad-linear-model.png)

If we look at a summary of the linear model, we can see that the residuals aren't normally distributed.

```{r bad_model, echo=FALSE}
bad_model <- lm(formula = y ~ x, data = bad_data)
summary(bad_model)
```

Notice that the slope (Estimate column) is near zero (`r round(bad_model$coefficients[[2]], digits = 6)`), indicating no relationship between the two variables.

The histogram for the residuals doesn't show a normal distribution, which is one of the three important assumptions for linear models. 

```{r bad_model_hist, echo=FALSE}
hist(x = bad_model$residuals, breaks=30)
```

In the Q-Q plot most of the data points are off-diagonal. If we compare the Q-Q plot for the bad model with our earlier model, we can see that most of the residuals fall well outside of the confidence interval.

```{r bad_model_qqplot, echo=FALSE}
par(mfcol = c(1, 2))
qqPlot(model, main = "Body Weight as a Function of Diet")
qqPlot(bad_model, main = "A Bad Model")
```

To produce Q-Q plots with confidence intervals (shown in dashed blue lines), use the `qqPlot()` function in the `car` library. The default argument for the confidence interval is `envelope=.95` for a 95% confidence interval. You can change this argument to a greater or lesser value, or you can set it to `FALSE` for no confidence interval around the diagonal.

```{r bad_model_v_good, echo=FALSE}
par(mfcol = c(1, 2))
plot(model, which = 1)
plot(bad_model, which = 1)
```

The good model is shown at left and the bad model at right above. Notice that the residuals for the good model are centered very close to zero and have an equal spread above and below zero. For the bad model at right, the residuals aren't centered at zero and don't appear to be equally spread above and below.

## Outliers

If outliers are present in the data, data transformation might be a consideration.  Alternatively, if outlier values are believed to be the result of real error (e.g. calculation error, data entry, etc.) then they may be removed from the dataset.  Excluding values must only be done for legitimate reasons, or else you may affect the Type 1 (false positive) error rate.

Include plot of distribution of data (include R box plot code).  Highlight any outliers.  Go over how to read a box plot.  Show example of outliers if none are present in data.

## A Model of Body Weight, Sex, and Diet

Let's return to the body weight data to evaluate a model of body weight as a function of both diet and sex. Evaluating the assumptions of the statistical test requires that a model be created.  A statistical model is a mathematical representation of the factors that can be used to predict a certain value.  For example, in our effort to predict body weight at 10 weeks, we will use the sex and diet of mice which are represented in the form of a statistical model.

The first step in analyzing data is to create an appropriate model.  Given our data set, we would like to determine if the dependent variable of body weight (BW.10) is influenced by the independent variables sex and diet.  The model for this analysis is:

![](../fig/bw-sex-diet-eqn.png)

where the subscript <i>i</i> refers to the individual sample. The mathematical function consists of two main parts. These parts are known as the predictor variables (or regressors), e.g., <i>sex<sub>i</sub></i>,… , and the parameters (or regression coefficients), e.g., <i>&beta;<sub>1</sub></i>,…. Like the parameters (or regression coefficients) in the mathematical function, the random errors are unknown. The error (or residual) is simply the difference between what is seen in the data set versus what is predicted by the mathematical function.

&nbsp;&nbsp;&nbsp;<i>BW.10<sub>i</sub></i> = the dependent (or response variable), body weight at 10 weeks, associated with sample <i>i</i>.

&nbsp;&nbsp;&nbsp;<i>&beta;<sub>0</sub></i> = mean intercept (or constant); for scientific studies the intercept is often not of interest and is only used to aid in calculation of predicted values. In our first linear model of body weight and diet, <i>&beta;<sub>0</sub></i> (the y-intercept) equaled 25.9. It's not meaningful, however, because it predicts that a mouse on neither diet has a body weight of 25.9.

&nbsp;&nbsp;&nbsp;<i>&beta;<sub>1</sub></i> = parameter associated with the regressor Sex. This number is a constant like the value of 2.6 in the first linear model that we explored.

&nbsp;&nbsp;&nbsp;<i>&beta;<sub>2</sub></i> = parameter associated with the regressor Diet; also a constant.

&nbsp;&nbsp;&nbsp;<i>sex<sub>i</sub></i> = a regressor that varies according to the ith sample’s sex (male or female)

&nbsp;&nbsp;&nbsp;<i>diet<sub>i</sub></i> = a regressor that varies according to the ith sample’s diet (standard chow or high-fat)

&nbsp;&nbsp;&nbsp;<i>&epsilon;<sub>i</sub></i> = error (or residual) associated with observation i

The response variable, <i>BW.10</i>, is a quantity that varies in a way that we hope to be able to summarize and exploit via the modeling process. Generally, it is known that the variation of the response variable is systematically related to the values of one or more other variables (such as, Sex and Diet) before the modeling process is begun, although testing the existence and nature of this dependence is part of the modeling process itself.

```{r linear_model_2}
# Model body weight at 10 weeks as a function of sex and diet
model2 <- lm(formula = BW.10 ~ Sex + Diet, data = pheno)
summary(model2)
# Look at the first few values of the residuals
head(model2$residuals)
# Look at the first few values fitted by the model
head(model2$fitted.values)
```

```{r histogram2}
hist(model2$residuals, breaks = 20)
```

Check for normal distribution of the residuals with a Q-Q plot. 

```{r qq_plot2}
plot(model2, which = 2)
```



Plot the residuals against the fitted values in this second model.

```{r resid_vs_fitted2}
plot(model2, which = 1)
```

Note that there are now 4 different groups - one for each combination of sex and diet.


Include R example of using aov() to setup an analysis of variance to predict BW.10 using both Sex and Diet factors.  May wish to mention the use of “lme” when you need to account for both fixed and random factors, such as when a random term is required for accounting for technical replicates (or other factors).