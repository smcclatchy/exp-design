---
layout: reference
permalink: /reference/
---

## Glossary

{:auto_ids}
blocking
:    Samples of similar structure grouped together from both treatment and control.

biological replicate
:   Measurement(s) from biologically distinct samples (preferably taken at the same time) that convey the random biological variation that exists within a population.  Biological replicates should not be confused with technical replicates.

block
:

blocking
: used to reduce unexplained variability by grouping together samples of similar structure from both treatment and control. For example,
a new drug is tested on both male and female subjects. Sex of the patient is a blocking factor that accounts for treatment variability between males and females.

confounder
: An unaccounted for variable that exerts either a small or large effect on a dependent (response) variable.  Such variables increase variance and bias in the study.

control
: An experimental subject that does not receive the treatment, and that is used as a baseline to evaluate the effect of the treatment on another group of subjects.

controlled experiment
: an experiment done in parallel on a treatment and a control group that differ in one way (the independent or explanatory variable). Investigators determine which subjects go in the treatment group and which in the control group. For contrast, see observational experiment.

deviation
:

effects
:

experimental error
:

experimental unit
:

factorial design
:   A design that permits testing of multiple variables at once.

fixed effects
:

observational experiment
: an experiment done in parallel on a treatment (or exposure) and a control group that differ in one way (the independent or explanatory variable). The subjects, not the investigators, determine whether they are in the treatment group or the control group (i.e. smokers and non-smokers). For contrast, see controlled experiment.

random effects
:

random error
:

randomization
:    A method to reduce bias and minimize the likelihood of chance altering the results of an experiment.

replicability
:   Other researchers obtain corroborating results using experimental methods similar to those of the original study,
    generating their own data independently.

reproducibility
:   Other researchers duplicate results and can draw the same conclusions as the original study did using
    the same materials and methods (i.e. specific measurement devices, original data, software, statistical method).

sensitivity
:

specificity
:

systematic error
:

technical replicate
:   Repeated measurements of the same sample that represent independent measures of the random noise
    associated with protocols or equipment. For contrast, see biological replicate.

treatment
:

variability
: the extent to which a distribution is spread out or squeezed in; also known as dispersion or spread

variance
: a measure of statistical variability or dispersion

variation
: 

## External references

### Data source
Gatti, DM, Simecek P, Somes L, Jeffery CT, Vincent MJ, Choi K, Chen X, Churchill GA and Svenson KL (2017).
[The Effects of Sex and Diet on Physiology and Liver Gene Expression in Diversity Outbred Mice.](https://www.biorxiv.org/content/early/2017/01/05/098657.full.pdf+html) bioRxiv.

### Reproducibility
Collins FS, Tabak LA. [Policy: NIH plans to enhance reproducibility.](https://www.nature.com/news/policy-nih-plans-to-enhance-reproducibility-1.14586) Nature. 2014 Jan 30;505(7485):612-3.

Goodman SN, Fanelli D and Ioannidis, JPA. [What does research reproducibility mean?](http://stm.sciencemag.org/content/8/341/341ps12.full) Science Trans Med. 2016 Jun 01;8(341)

Hess KR. [Statistical design considerations in animal studies published recently in cancer research.](https://cancerres.aacrjournals.org/content/canres/71/2/625.full.pdf) Cancer research. 2011 Jan 15;71(2):625-.

National Academies of Sciences, Engineering, and Medicine. 2015. [Reproducibility Issues in Research with Animals and Animal Models: Workshop in Brief](https://www.nap.edu/read/21835/). Washington, DC: The National Academies Press. https://doi.org/10.17226/21835.

National Academies of Sciences, Engineering, and Medicine. 2019. [Reproducibility and Replicability in Science](https://www.nap.edu/catalog/25303/reproducibility-and-replicability-in-science).  Washington, DC: The National Academies Press. https://doi.org/10.17226/25303.

Chalmers, Iain et al. 2009. [Avoidable waste in the production and reporting of research evidence](https://www.thelancet.com/journals/lancet/article/PIIS0140673609603299/fulltext?rss=yes). The Lancet 374(9683):86-89

Bollen, K, Cacioppo, JT, Kaplan, R, Krosnick, J, Olds, JL. 2015. [Social, Behavioral, and Economic Sciences Perspectives on Robust and Reliable Science](https://www.nsf.gov/sbe/AC_Materials/SBE_Robust_and_Reliable_Research_Report.pdf). Arlington, VA: National Science Foundation.

Nuzzo, R. [How scientists fool themselves – and how they can stop](https://www.nature.com/news/how-scientists-fool-themselves-and-how-they-can-stop-1.18517). Nature 526(7572) 182–185 (08 October 2015)

Yong, E. [Replication studies: Bad copy](https://www.nature.com/news/replication-studies-bad-copy-1.10634). Nature 485, 298–300. (17 May 2012)


### Experimental design
Krzywinski M, Altman N. [Designing comparative experiments.](https://www.nature.com/articles/nmeth.2974) Nat Methods. 2014 June;11(6):597-598.

Blainey P, Krzywinski M, Altman N. [Points of significance: replication.](https://www.nature.com/articles/nmeth.3091) Nat Methods. 2014 Sep;11(9):879-80.

Voelkl, B., Würbel, H., Krzywinski, M. et al. [The standardization fallacy.](https://www.nature.com/articles/s41592-020-01036-9) Nat Methods 18, 5–7 (2021). https://doi.org/10.1038/s41592-020-01036-9

[ILAR Journal: Design & Statistical Analysis of Animal Experiments](https://academic.oup.com/ilarjournal/issue/55/3)

Dickersin K, Chan SS, Chalmersx TC, Sacks HS, Smith Jr H. Publication bias and clinical trials. Controlled clinical trials. 1987 Dec 1;8(4):343-53.

Error prone. Nature  487  406 EP -  (2012) https://doi.org/10.1038/487406a

[Festing & Altman's Guidelines for the Design and Statistical Analysis of Experiments
Using Laboratory Animals](http://www.3rs-reduction.co.uk/assets/applets/Festing_Altman.pdf)

Fisher RA. The design of experiments.

Derek J. Fry, [Teaching Experimental Design, ILAR Journal, Volume 55, Issue 3, 2014, Pages 457–471,](https://doi.org/10.1093/ilar/ilu031)

Johnson PD, Besselsen DG. [Practical aspects of experimental design in animal research.](http://www.bbf.uns.edu.ar/files/disenoexperimental.pdf) ILAR journal. 2002 Jan 1;43(4):202-6.

Kilkenny C, Parsons N, Kadyszewski MF, Cuthill IC, Fry D, Hutton J, Altman DG. Survey of the quality of experimental design, statistical analysis and reporting of research using animals. PloS one. 2009;4(11).

Kilkenny C, Browne WJ, Cuthill IC, Emerson M, Altman DG. Improving bioscience research reporting: the ARRIVE guidelines for reporting animal research. PLoS biology. 2010 Jun;8(6).

[Gary Oehlert's A First Course in Design and Analysis of Experiments](http://users.stat.umn.edu/~gary/book/fcdae.pdf)

[Statistics Done Wrong by Alex Reinhart](https://www.statisticsdonewrong.com/index.html)

[Repeated Measures Design by Mark Conaway](http://biostat.mc.vanderbilt.edu/wiki/pub/Main/ClinStat/repmeas.PDF)

[The Analysis Factor: Effect Size Statistics, Power, and Sample Size Calculations](https://www.theanalysisfactor.com/resources/by-topic/effect-size-statistics-power-and-sample-size-calculations/)

[The Analysis Factor: Series on Confusing Statistical Terms](http://www.theanalysisfactor.com/series-on-confusing-statistical-terms/)

[Statistical Rules of Thumb](https://www.amazon.com/Statistical-Rules-Thumb-Gerald-Belle/dp/0470144483)

[Nature Practical Guides](https://www.nature.com/collections/qghhqm/content/practical-guides)

Krzywinski M, Altman N. [Points of significance: Analysis of variance and blocking.](https://www.nature.com/articles/nmeth.3005) Nat Methods. 2014 Jul;11(7):699-700.

[Why animal research needs to improve](https://www.nature.com/news/2011/110928/full/477511a.html) 28 September 2011 | Nature 477, 511 (2011) | doi:10.1038/477511a

Optimal experimental design
B. Smucker and M. Krzywinski and N. Altman
Nature Methods  15  559--560  (2018)
https://doi.org/10.1038/s41592-018-0083-2
Customize the experiment for the setting instead of adjusting the setting to fit a classical design.

Thabane L, Ma J, Chu R, Cheng J, Ismaila A, Rios LP, Robson R, Thabane M, Giangregorio L, Goldsmith CH. [A tutorial on pilot studies: the what, why and how.](https://bmcmedresmethodol.biomedcentral.com/articles/10.1186/1471-2288-10-1) BMC medical research methodology. 2010 Dec;10(1):1.

Turner EH, Matthews AM, Linardatos E, Tell RA, Rosenthal R. [Selective publication of antidepressant trials and its influence on apparent efficacy.](https://www.nejm.org/doi/pdf/10.1056/nejmsa065779) New England Journal of Medicine. 2008 Jan 17;358(3):252-60.

### Statistics
Face up to false positives
D. MacArthur
Nature  487  427 EP -  (2012)
https://doi.org/10.1038/487427a

Nuzzo, R. [Scientific method: Statistical errors](https://www.nature.com/news/scientific-method-statistical-errors-1.14700).     Nature 506, 150–152. (13 February 2014)

Know when your numbers are significant
D. L. Vaux
Nature  492  180 EP -  (2012)
https://doi.org/10.1038/492180a

